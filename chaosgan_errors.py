# -*- coding: utf-8 -*-
# chaosgan_errors.py
# Analysis of errors of time series generated by chaosgan
#%%
from models import BATCH_SIZE, NOISE_DIM
from nltsa import delay_embedding  # , wayland, e_wayland
from tools import random_x0, TrajGenerator, TransitionErrors
from models import ChaosGANTraining
from chaosmap import f_henon, f_logistic, f_ikeda, f_tent, iterate_f_batch, iterate_f_batch_with_noise
from sklearn.neighbors import KDTree
import datetime
import scipy
import scipy.stats
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

import time
from IPython import display
import os

# import other files
import importlib
import chaosmap
import models
import parzen

importlib.reload(chaosmap)
importlib.reload(models)
importlib.reload(parzen)

# fixing random seed
np.random.seed(100)
tf.random.set_seed(101)

plt.rcParams["savefig.dpi"] = 300


def add_log(message, path, display=True):
  # add to log file

  if not os.path.exists(path):
    opt = 'w'
  else:
    opt = 'a'

  with open(path, opt) as f:
    print(message, file=f)
  if display:
    print(message)

# %%


#what kind of errors are used
er_sig = 'transition'  # d_er as d


GEN_IMAGES_DIR = 'gen_images'
output_length_about = 1000
S_NOISE = 1e-14


# set map name here
# TODO: use argparse
mapname = 'logistic'  # logistic, tent, henon or ikeda
checkpoint_dir = os.path.join('training_checkpoints', mapname)
save_image_dir = os.path.join(GEN_IMAGES_DIR, mapname)
saved_data_dir = os.path.join('saved_data', mapname)
result_logfile = os.path.join('saved_data', mapname, 'error_result.log')

#%%
if __name__ == "__main__":
  # GPU memory setting
  physical_devices = tf.config.experimental.list_physical_devices('GPU')
  if len(physical_devices) > 0:
    for k in range(len(physical_devices)):
        tf.config.experimental.set_memory_growth(physical_devices[k], True)
        print('memory growth:', tf.config.experimental.get_memory_growth(
            physical_devices[k]))
  else:
    print("Not enough GPU hardware devices available")

  os.makedirs(save_image_dir, exist_ok=True)
  os.makedirs(saved_data_dir, exist_ok=True)

  import matplotlib as mpl

  def is_env_notebook():
    """Determine whether is the environment Jupyter Notebook"""
    if 'get_ipython' not in globals():
        # Python shell
        return False
    from IPython import get_ipython
    env_name = get_ipython().__class__.__name__
    if env_name == 'TerminalInteractiveShell':
        # IPython shell
        return False
    # Jupyter Notebook
    return True

  if not is_env_notebook():
    print('use AGG')
    mpl.use('Agg')  # off-screen use of matplotlib
#%%
dim_map = 0
f_map = ''
f_param = ''
x0min = 0
x0max = 0

add_log(
    f'Adding log message of chaosgan_errors.py\n{datetime.datetime.today()}', result_logfile)

if mapname == 'ikeda':
  f_map = f_ikeda
  dim_map = 2
  x0min = np.array([-0.1, -0.1])
  x0max = np.array([0, 0])
elif mapname == 'logistic':
  f_map = f_logistic
  dim_map = 1
  x0min = np.array([0.0])
  x0max = np.array([1.0])
elif mapname == 'henon':
  f_map = f_henon
  dim_map = 2
  x0min = np.array([-0.1, -0.1])
  x0max = np.array([0.1, 0.1])
elif mapname == 'tent':
  f_map = f_tent
  dim_map = 1
  x0min = np.array([0.0])
  x0max = np.array([1.0])

cgantr = ChaosGANTraining(batch_size=BATCH_SIZE, noise_dim=NOISE_DIM,
                          output_length_about=output_length_about, dim_map=dim_map)

generator, discriminator = cgantr.make_models()
generator_optimizer, discriminator_optimizer = cgantr.set_optimizer()
cgantr.set_checkpoint(checkpoint_dir)
# set image dir
cgantr.set_image_dir(GEN_IMAGES_DIR)
# tmax = generator.output_shape[1]-1

# %%　
cgantr.build_model_by_input()

# %% making dataset


def param_ikeda(bs): return (1.0, 0.4, 0.9, 6.0)  # Ikeda map
def param_henon(bs): return (1.2, 0.4)  # Henon map
def param_tent(bs): return (2.0,)  # Tent map


def gen_as(bs, low=4.0, high=4.0):
    '''return parameter values for logistic maps'''
    return np.random.uniform(low=low, high=high, size=(1, bs))


use_noise = False
s_n = 0
if mapname == 'ikeda':
  f_param = param_ikeda
elif mapname == 'logistic':
  f_param = gen_as
elif mapname == 'henon':
  f_param = param_henon
elif mapname == 'tent':
    f_param = param_tent
    use_noise = True
    s_n = S_NOISE

trjgen = TrajGenerator(batch_size=BATCH_SIZE, dim=dim_map,
                       f=f_map, tmax=cgantr.tmax, gen_param=f_param, transient=100,
                       use_noise=use_noise, s_noise=s_n,
                       x0min=x0min, x0max=x0max)
AUTOTUNE = tf.data.experimental.AUTOTUNE

ds = tf.data.Dataset.from_generator(
    trjgen, output_types=(tf.float64, tf.float64)).prefetch(buffer_size=AUTOTUNE)

# fixed seed for sampling
seed = tf.random.normal(cgantr.input_shape)


#%% restore model
cgantr.checkpoint.restore(tf.train.latest_checkpoint(cgantr.checkpoint_dir))

# %%

# long sequences
bs = 1
transient = 1000
tmax = 100000
t_total = transient + tmax


if mapname == 'logistic':
  params = 4.0  # value of a
elif mapname == 'ikeda':
  def param_ikeda(bs): return (1.0, 0.4, 0.9, 6.0)  # Ikeda
  params = param_ikeda(bs)
elif mapname == 'henon':
  def param_henon(bs): return (1.2, 0.4)  # Henon
  params = param_henon(bs)
elif mapname == 'tent':
  def param_tent(bs): return (2.0,)  # Tent
  params = param_tent(bs)
else:
  params = None

x0 = random_x0((bs, dim_map), x0min, x0max).transpose(
    [1, 0])  # (dim_map, bs) #type:ignore

if use_noise:
  x = iterate_f_batch_with_noise(
      x0, f_map, tmax=tmax, transient=transient, param=params, s=S_NOISE)
else:
  x = iterate_f_batch(
      x0, f_map, tmax=tmax, transient=transient, param=params)
x = x.reshape([-1, dim_map])

init_shape = [1, tmax // 8 + 13, 64]
seed = tf.random.normal(init_shape)
X_sample = generator(seed, training=False).numpy()
print(f'X_sample.shape: {X_sample.shape}')
x_sample = X_sample[0, :tmax + 1, :]  # change to 2dim-array

xd = delay_embedding(x, 2, 1)  # 埋め込み
xd_sample = delay_embedding(x_sample, 2, tau=1)  # embedding

print(xd.shape)
print(xd_sample.shape)

# %%　checking
plt.rcParams["font.size"] = 18
fig,ax = plt.subplots()
if dim_map == 1:
  ax.plot(x_sample[0:-1, 0], x_sample[1:, 0], '.', alpha=1, ms=1)
  ax.plot(x[0:-1, 0], x[1:, 0], '.', ms=2)
elif dim_map == 2:
  ax.plot(x_sample[:, 0], x_sample[:, 1], '.', alpha=1, ms=1)
  ax.plot(x[:, 0], x[:, 1], '.', ms=0.5)
ax.set_xlabel('x(n)')
ax.set_ylabel('x(n+1)')
fig.tight_layout()

#%%
# Transition Error
errors = TransitionErrors(f=f_map)
error_summary = errors(X_sample, p=np.array(f_param(bs=bs)))  # type:ignore

er = errors.er.reshape((-1, dim_map))
er_abs = np.abs(errors.er).reshape((-1, dim_map))
d_er = np.sqrt(np.sum(er_abs**2, axis=1))
rmse = np.sqrt(np.mean(np.sum(er_abs**2, axis=1)))
plt.plot(d_er)
plt.rcParams["font.size"] = 16
#%%
d = d_er

#%%
fig, ax = plt.subplots()
ax.plot(d)
ax.set_xlabel(r'$t$')
ax.set_ylabel(r'$d_t$')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_timeseries_all.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_timeseries_all.pdf"))

plt.show()
#%%
fig, ax = plt.subplots()
ax.plot(d[0:2000])
ax.set_xlabel(r'$t$')
ax.set_ylabel(r'$d_t$')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_timeseries_short.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_timeseries_short.pdf"))
plt.show()

fig, ax = plt.subplots()
ax.plot(d[0:500])
ax.set_xlabel(r'$t$')
ax.set_ylabel(r'$d_t$')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir,
            "d_timeseries_very_short.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_timeseries_very_short.pdf"))

# %%
# square mean of error are used as variance of Gaussian function for fitting
sigma_d = np.sqrt(np.mean(d**2))  #
add_log(f'sigma_d= {sigma_d}', result_logfile)

# %%  checking stability
var_cum = np.cumsum(d**2) / (1 + np.arange(d.shape[0]))
fig, ax = plt.subplots()
ax.plot(var_cum)  # checking convergence
# %% to z-value
z_d = np.squeeze(d / sigma_d)
fig, ax = plt.subplots()
ax.plot(z_d)
# %% histogram of d
d_density, d_bin, batches = plt.hist(d, bins=1000, density=True)

############
#   drawing complementary cumulative distribution function
############
#%%
sd = np.sort(d)  # ascending order
d_rank = (np.array(range(sd.size)) + 1)
# complementary cumulative distribution function(CCDF) of Gaussian distribution.
P_d = 1 - (d_rank - 1) / d_rank.size

norm_dist = scipy.stats.distributions.norm(scale=sigma_d)
# norm_dist = scipy.stats.distributions.norm(scale=0.013)
# CCDF of Gaussian distribution fitted to data
P_norm_x = 1 - 2 * (norm_dist.cdf(sd) - 0.5)

# %% fitting of exponential distribution
loc, scale = scipy.stats.distributions.expon.fit(d, floc=0)
exp_dist = scipy.stats.distributions.expon(scale=scale)
P_exp = exp_dist.sf(sd)  # CCDF of exponential distribution fitted to data

# %% fitting of log-normal distribution
s, loc, scale = scipy.stats.distributions.lognorm.fit(d, floc=0)
lognorm_dist = scipy.stats.distributions.lognorm(s, loc=loc, scale=scale)
P_lognorm = lognorm_dist.sf(sd)  # CCDF of log-normal distribution

# %% fitting of gamma distribution
a, loc, scale = scipy.stats.distributions.gamma.fit(d, floc=0)
gamma_dist = scipy.stats.distributions.gamma(a, loc=loc, scale=scale)
P_gamma = gamma_dist.sf(sd)  # CCDF of gamma distribution


# %% to z-value. removing outliers (this analysis was not used in the paper)
z_d = (d - np.mean(d))/np.std(d)

# remove z>3
d_tranc = d[z_d < 3.0]
print(d_tranc.shape)
sigma_d_tranc = np.sqrt(np.mean(d_tranc**2))  #
print(f'sigma_d= {sigma_d_tranc}')
norm_dist_tranc = scipy.stats.distributions.norm(scale=sigma_d_tranc)

P_norm_tranc = 1 - 2 * (norm_dist_tranc.cdf(sd) - 0.5)


# %%　plottings of CCDF of empirical data and fitted distributions

fig, ax = plt.subplots()
ax.plot(sd, P_d, '.-', label='data')
ax.plot(sd, P_norm_x, label='normal')
# ax.plot(sd, P_norm_tranc, label='normal_tranc')
ax.plot(sd, P_exp, label='exponential')
ax.plot(sd, P_lognorm, ':', label='log-normal')
ax.plot(sd, P_gamma, ':', label='gamma')
ax.set_xlabel(r'$d$')
ax.set_ylabel(r'$\bar{P}(d)$')
ax.legend()
#ax.set_yscale('log')
#ax.set_xscale('log')
# ax.set_ylim([1e-5, 10])
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_ccdf_normal.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_ccdf_normal.pdf"))

plt.show()
#%%
fig, ax = plt.subplots()
ax.plot(sd, P_d, '.-', label='data')
ax.plot(sd, P_norm_x, '--', label='normal')
# ax.plot(sd, P_norm_tranc, ':', label='normal_tranc')
ax.plot(sd, P_exp, '-.', label='exponential')
ax.plot(sd, P_lognorm, ':', label='log-normal')
ax.plot(sd, P_gamma, ':', label='gamma')
ax.set_xlabel(r'$d$')
ax.set_ylabel(r'$\bar{P}(d)$')
ax.legend()
ax.set_yscale('log')
#ax.set_xscale('log')
ax.set_ylim([1e-5, 10])
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_ccdf_semilog.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_ccdf_semilog.pdf"))
fig.show()

fig, ax = plt.subplots()
ax.plot(sd, P_d, '.-', label='data')
ax.plot(sd, P_norm_x, '--', label='normal')
# ax.plot(sd, P_norm_tranc, ':', label='normal_tranc')
ax.plot(sd, P_exp, '-.', label='exponential')
ax.plot(sd, P_lognorm, ':', label='log-normal')
ax.plot(sd, P_gamma, ':', label='gamma')
ax.set_xlabel(r'$d$')
ax.set_ylabel(r'$\bar{P}(d)$')
ax.legend()
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_ylim([1e-5, 10])
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_ccdf_loglog.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_ccdf_loglog.pdf"))

plt.show()


# %% density functions
log_bin = np.logspace(np.log10(0.001), np.log10(1.0), 200)
d_density, d_bin, batches = plt.hist(
    d, bins=log_bin, density=True)  # type:ignore

#%% truncated
d_density_tranc, d_bin_tranc, batches_tranc = plt.hist(
    d_tranc, bins=log_bin, density=True)  # type:ignore
# %%
fig, ax = plt.subplots()
ax.plot(d_bin[1:], d_density, '.', label='data')
ax.plot(d_bin, 2 * norm_dist.pdf(d_bin), label='normal')
# ax.plot(d_bin, 2 * norm_dist_tranc.pdf(d_bin), label='normal_tranc')
ax.plot(d_bin, exp_dist.pdf(d_bin), label='exp')
ax.plot(d_bin, lognorm_dist.pdf(d_bin), label='log-normal')
ax.plot(d_bin, gamma_dist.pdf(d_bin), label='gamma')
ax.legend()
ax.set_xlabel(r'$d$')
ax.set_ylabel('density')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_pdf_1.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_pdf_1.pdf"))

plt.show()
#%%
fig, ax = plt.subplots()
ax.plot(d_bin[1:], d_density, '.', label='data')
ax.plot(d_bin, 2 * norm_dist.pdf(d_bin), label='normal')
# ax.plot(d_bin, 2 * norm_dist_tranc.pdf(d_bin), label='normal_tranc')
ax.plot(d_bin, exp_dist.pdf(d_bin), label='exp')
ax.plot(d_bin, lognorm_dist.pdf(d_bin), label='log-normal')
ax.plot(d_bin, gamma_dist.pdf(d_bin), label='gamma')
fig.legend()
ax.set_xlim([-0.01, 0.1])# type:ignore
ax.set_xlabel(r'$d$')
ax.set_ylabel('density')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_pdf_2.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_pdf_2.pdf"))
fig.show()

fig, ax = plt.subplots()
ax.plot(d_bin[1:], d_density, '.', label='data')
ax.plot(d_bin, 2 * norm_dist.pdf(d_bin), label='normal')
# ax.plot(d_bin, 2 * norm_dist_tranc.pdf(d_bin), label='normal_tranc')
ax.plot(d_bin, exp_dist.pdf(d_bin), label='exp')
ax.plot(d_bin, lognorm_dist.pdf(d_bin), label='log-normal')
ax.plot(d_bin, gamma_dist.pdf(d_bin), label='gamma')
ax.legend()
# ax.set_xlim([-0.01, 0.1])
ax.set_xlabel(r'$d$')
ax.set_ylabel('density')
ax.set_yscale('log')
ax.set_ylim([1e-5, 100])
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_pdf_semilog.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_pdf_semilog.pdf"))
plt.show()

#%%
fig, ax = plt.subplots()
ax.plot(d_bin[0:-1], d_density, '.', label='data')
ax.plot(d_bin, 2 * norm_dist.pdf(d_bin), label='normal')
# ax.plot(d_bin, 2 * norm_dist_tranc.pdf(d_bin), label='normal_tranc')
ax.plot(d_bin, exp_dist.pdf(d_bin), label='exp')
ax.plot(d_bin, lognorm_dist.pdf(d_bin), label='log-normal')
# ax.plot(d_bin, gamma_dist.pdf(d_bin), label='gamma')
ax.legend()
# ax.set_xlim([-0.01, 0.1])
ax.set_xlabel('d')
ax.set_ylabel('density')
ax.set_yscale('log')
ax.set_xscale('log')
ax.set_ylim([1e-5, 100])
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_pdf_loglog.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_pdf_loglog.pdf"))

############
#% autocorrelation function of d
# purpose: check if there is no periodicity in d (check if there is no periodicity due to filter size or upsampling)
############

d_nom = (d - d.mean())/d.std()  # normalization
pow_d = np.abs(np.fft.fft(d_nom))**2  # power spectrum
# calculating autocorrelation function by ifft. normalize by the number of elements.
ac_d = np.abs(np.fft.ifft(pow_d))/d_nom.shape[0]

fig, ax = plt.subplots()
ax.plot(ac_d[0:100], '.-')
ax.set_xlabel('s')
ax.set_ylabel('auto-correlation')
ax.grid()
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "d_autocorrelation.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "d_autocorrelation.pdf"))

# %%  Statistics of fragment length

if (mapname == 'logistic') or (mapname == 'tent'):
  threshold = 0.1
elif (mapname == 'ikeda') or (mapname == 'henon'):
  threshold = 0.4
else:
  threshold = 0.0
n_sample_seq = 100


def gen_fragment_length(threshold, n_sample_seq=100, tmax=100000):
  '''
    doing sampling multiple times and collecting sampling of fragment length 
    args:
      threshold: threshold value for fragment length
      n_sample_seq: number of sampling sequences
  '''

  len_fr = []
  x0 = random_x0((bs, dim_map), x0min, x0max).transpose(
      [1, 0])  # (dim_map, bs)
  if use_noise:
    x = iterate_f_batch_with_noise(
        x0, f_map, tmax=tmax, transient=transient, param=params, s=S_NOISE)
  else:
    x = iterate_f_batch(
        x0, f_map, tmax=tmax, transient=transient, param=params)
  x = x.reshape([-1, dim_map])
  xd = delay_embedding(x, 2, 1)  # teaching data
  tree = KDTree(xd, leaf_size=2)  # tree

  for n in range(n_sample_seq):
    seed = tf.random.normal(init_shape)  # random seed
    X_sample = generator(seed, training=False).numpy()  # generate sequence
    x_sample = X_sample[0, :tmax + 1, :]  # change to 2dim-array (t_len, dim)
    xd_sample = delay_embedding(x_sample, 2, tau=1)  # delay-embedding
    d = np.array(0.0)
    errors = TransitionErrors(f=f_map)
    error_summary = errors(
        X_sample, p=np.array(f_param(bs=bs)))  # type:ignore

    er = errors.er.reshape((-1, dim_map))
    er_abs = np.abs(errors.er).reshape((-1, dim_map))
    d_er = np.sqrt(np.sum(er_abs**2, axis=1))
    d = d_er

    fr_ind = np.where(d > threshold)  # position of values more than threshold
    # calculate length of fragment by taking difference of adjacent indices
    l_fr = np.diff(fr_ind[0])
    len_fr.append(l_fr)
    # print(n)

  len_fr = np.concatenate(len_fr, axis=0)
  return len_fr


len_fr = gen_fragment_length(threshold, n_sample_seq, tmax=tmax)
#ax.plot(l_fr)

#%% plotting points of large error

if use_noise:
  x = iterate_f_batch_with_noise(
      x0, f_map, tmax=tmax, transient=transient, param=params, s=S_NOISE)
else:
  x = iterate_f_batch(
      x0, f_map, tmax=tmax, transient=transient, param=params)
x = x.reshape([-1, dim_map])
xd = delay_embedding(x, 2, 1)  # teaching data

seed = tf.random.normal(init_shape)  # random seed
X_sample = generator(seed, training=False).numpy()  # generate sequence
x_sample = X_sample[0, :tmax + 1, :]  # change to 2dim-array (t_len, dim)
xd_sample = delay_embedding(x_sample, 2, tau=1)  # delay-embedding


errors = TransitionErrors(f=f_map)
error_summary = errors(X_sample, p=np.array(f_param(bs=bs)))  # type:ignore

er = errors.er.reshape((-1, dim_map))
er_abs = np.abs(errors.er).reshape((-1, dim_map))
d_er = np.sqrt(np.sum(er_abs**2, axis=1))
d = d_er

fr_ind = np.where(d > threshold)

fig, ax = plt.subplots()
if dim_map == 1:
  ax.plot(xd_sample[:, 0], xd_sample[:, 1], '.', ms=2)
  ax.plot(xd_sample[fr_ind[0], 0], xd_sample[fr_ind[0], 1], 'x', ms=4)
else:
  ax.plot(xd_sample[:, dim_map], xd_sample[:, dim_map+1], '.', ms=2)
  ax.plot(xd_sample[fr_ind[0], dim_map],
          xd_sample[fr_ind[0], dim_map+1], 'x', ms=4)

ax.text(0.8, 0.8, fr'$\lambda={threshold:.2}$')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "frl_large_d.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "frl_large_d.pdf"))

# %% Histogram
fig, ax = plt.subplots()
count, bin, batches = ax.hist(len_fr, bins=100, density=False)
ax.set_xlabel('length')
ax.set_ylabel('count')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "frl_hist.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "frl_hist.pdf"))

# %% semilog scale
fig, ax = plt.subplots()
ax.plot(bin[1:], count, '.-')
# plt.xscale('log')
ax.set_xlabel('length', fontsize=18)
ax.set_ylabel('count', fontsize=18)
ax.set_yscale('log')
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir, "frl_hist_semilog.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "frl_hist_semilog.pdf"))

# %% removing length 1
len_fr_mod = len_fr[len_fr > 4]
count, bin, batches = plt.hist(len_fr_mod, bins=100, density=False)
#%%
fig, ax = plt.subplots()
ax.plot(bin[1:], count, '.-')
ax.set_xlabel('length')
ax.set_ylabel('count')
# ax.set_xscale('log')
ax.set_yscale('log')
m_fr_mod = np.mean(len_fr_mod)
print(m_fr_mod)
fig.tight_layout()
fig.savefig(os.path.join(save_image_dir,
            "frl_hist_semilog_remove4.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "frl_hist_semilog_remove4.pdf"))


# %% trying analysis in various threshold
if (mapname == 'logistic') or (mapname == 'tent'):

  thresholds = [0.02, 0.05, 0.08, 0.1, 0.13, 0.15, 0.2]
else:
  thresholds = [0.05, 0.1, 0.2, 0.4, 0.5, 0.8]

n_sample_seq = 100  # number of sampling sequences

len_frs = []  # list of lists of fragment length
for _ in thresholds:
  len_frs.append([])
#%%
x0 = random_x0((bs, dim_map), x0min, x0max).transpose([1, 0])  # (dim_map, bs)
if use_noise:
  x = iterate_f_batch_with_noise(
      x0, f_map, tmax=tmax, transient=transient, param=params, s=S_NOISE)
else:
  x = iterate_f_batch(
      x0, f_map, tmax=tmax, transient=transient, param=params)


for n in range(n_sample_seq):
  seed = tf.random.normal(init_shape)
  X_sample = generator(seed, training=False).numpy()
  x_sample = X_sample[0, :tmax + 1, :]  # change to 2dim-array
  xd_sample = delay_embedding(x_sample, 2, tau=1)
  errors = TransitionErrors(f=f_map)
  error_summary = errors(X_sample, p=np.array(f_param(bs=bs)))  # type:ignore

  er = errors.er.reshape((-1, dim_map))
  er_abs = np.abs(errors.er).reshape((-1, dim_map))
  d_er = np.sqrt(np.sum(er_abs**2, axis=1))
  d = d_er

  # various threshold
  for list_fr, thr in zip(len_frs, thresholds):
    fr_ind = np.where(d > thr)
    l_fr = np.diff(fr_ind[0])
    list_fr.append(l_fr)
  # print(n)

for i in range(len(len_frs)):
  len_frs[i] = np.concatenate(len_frs[i], axis=0)

n_frs = np.array([len(ls) for ls in len_frs])  # number of total fragments

#%%
probs = []
bins = []

x_max_bin = 4000
if mapname == 'logistic' or mapname == 'tent':
  x_max_bin = 4000
else:
  x_max_bin = 2000

for i in range(len(thresholds)):
  bin_size = min(len_frs[i].max(), 100)
  bin = np.linspace(1, x_max_bin, 100)
  prob, bin, batches = plt.hist(
      len_frs[i], bins=bin, density=True)  # type:ignore
  probs.append(prob)
  bins.append(bin)


fig, ax = plt.subplots()
for i in range(len(thresholds)):
  lab = fr'$\lambda = {thresholds[i]:.3}$'
  ax.plot(bins[i][1:], probs[i], '.-', label=lab, ms=2)
ax.set_xlim((0, x_max_bin))
ax.legend()
ax.set_xlabel('fragment length', fontsize=18)
ax.set_ylabel('density', fontsize=18)
ax.set_yscale('log')
fig.tight_layout()


fig.savefig(os.path.join(save_image_dir, "frl_hist_thr_semilog.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "frl_hist_thr_semilog.pdf"))

fig, ax = plt.subplots(figsize=(7, 4))
stl = ['-', '-', '--', ':', '-.']

if mapname == 'logistic':
  th_subset = [1, 3, 5, 6]
else:
  th_subset = [0, 1, 2, 3]


for j, i in enumerate(th_subset):
  lab = fr'$\lambda = {thresholds[i]:.3}$'
  ax.plot(bins[i][1:], probs[i], stl[j], label=lab, ms=2)
ax.legend(loc='upper left', bbox_to_anchor=(1.05, 1.0))
ax.set_xlabel('fragment length')
ax.set_ylabel('density')
ax.set_yscale('log')
ax.set_xlim((0, x_max_bin/4))
fig.tight_layout()

fig.savefig(os.path.join(save_image_dir, "frl_hist_thr_semilog2.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "frl_hist_thr_semilog2.pdf"))

# %%
fig, ax = plt.subplots()
i = 3  # lambda = 0.1 for logistic map

lab = fr'$\lambda = {thresholds[i]:.3}$'
ax.plot(bins[i][1:], probs[i], '.-', label=lab, ms=2)
ax.set_xlim((0, 1200))
ax.legend()
ax.set_xlabel('fragment length', fontsize=18)
ax.set_ylabel('density', fontsize=18)
ax.set_yscale('log')
fig.tight_layout()

fig.savefig(os.path.join(save_image_dir, "frl_density_l01_semilog.png"), dpi=300)
fig.savefig(os.path.join(save_image_dir, "frl_density_l01_semilog.pdf"))
# %%
